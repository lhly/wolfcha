import { getLlmConfigWithDefaults } from "@/lib/llm-config";
import { getGeneratorModel, getSelectedModels } from "@/lib/api-keys";
import { type ModelRef } from "@/types/game";
import { gameStatsTracker } from "@/hooks/useGameStats";
import { gameSessionTracker } from "@/lib/game-session-tracker";

export type LLMContentPart =
  | { type: "text"; text: string; cache_control?: { type: "ephemeral"; ttl?: "1h" } }
  | { type: "image_url"; image_url: { url: string; detail?: string } }
  | { type: "input_audio"; input_audio: { data: string; format: "mp3" | "wav" } };

export type ApiKeySource = "user" | "project";

export interface LLMMessage {
  role: "system" | "user" | "assistant";
  content: string | LLMContentPart[];
  reasoning_details?: unknown;
}

export function resolveApiKeySource(model: string): ApiKeySource {
  const cfg = getLlmConfigWithDefaults();
  return cfg.apiKey ? "user" : "project";
}

export interface ChatCompletionResponse {
  id: string;
  choices: {
    message: {
      role: "assistant";
      content: string;
      reasoning_details?: unknown;
    };
    finish_reason: string;
  }[];
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export type ResponseFormat =
  | { type: "text" }
  | { type: "json_object" }
  | {
      type: "json_schema";
      strict?: boolean;
      json_schema: {
        name: string;
        description?: string;
        schema: unknown;
        // Note: 'strict' is not supported by ZenMux, use json_object for simple cases
      };
    };

// ZenMux reasoning: enabled, effort (minimal|low|medium|high), max_tokens (optional). No exclude.
export interface ReasoningOptions {
  enabled: boolean;
  effort?: "minimal" | "low" | "medium" | "high";
  max_tokens?: number;
}

export interface GenerateOptions {
  model: string;
  messages: LLMMessage[];
  temperature?: number;
  max_tokens?: number;
  reasoning?: ReasoningOptions;
  reasoning_effort?: "minimal" | "low" | "medium" | "high";
  response_format?: ResponseFormat;
}

/** Merge modelRef overrides (temperature, reasoning) into options; modelRef values override call-time when present. */
export function mergeOptionsFromModelRef<T extends GenerateOptions>(
  modelRef: ModelRef | undefined,
  options: T
): T {
  if (!modelRef) return options;
  const out = { ...options } as T;
  if (modelRef.temperature !== undefined) (out as GenerateOptions).temperature = modelRef.temperature;
  if (modelRef.reasoning !== undefined) (out as GenerateOptions).reasoning = modelRef.reasoning;
  return out;
}

export type BatchCompletionResult =
  | { ok: true; content: string; reasoning_details?: unknown; raw: ChatCompletionResponse }
  | { ok: false; error: string; status?: number };

const RETRYABLE_STATUS = new Set([429, 500, 502, 503, 504]);
const INVALID_MODEL_MARKER = "[INVALID_MODEL]";
const invalidModelCache = new Set<string>();

function parseRetryAfterMs(response: Response): number | null {
  const raw = response.headers.get("retry-after");
  if (!raw) return null;
  const sec = Number(raw);
  if (Number.isFinite(sec) && sec > 0) return Math.round(sec * 1000);

  const dateMs = Date.parse(raw);
  if (!Number.isFinite(dateMs)) return null;
  const diff = dateMs - Date.now();
  return diff > 0 ? diff : null;
}

const QUOTA_EXHAUSTED_MARKER = "[QUOTA_EXHAUSTED]";

function isQuotaExhaustedError(status: number, errorText: string): boolean {
  if (status === 402) return true;
  const lower = errorText.toLowerCase();
  return (
    lower.includes("insufficient") ||
    lower.includes("quota") ||
    lower.includes("balance") ||
    lower.includes("余额") ||
    lower.includes("欠费") ||
    lower.includes("arrearage") ||
    (status === 401 && lower.includes("已启用自定义 key"))
  );
}

function isInvalidModelError(status: number, errorText: string): boolean {
  if (status === 404) return true;
  const lower = errorText.toLowerCase();
  return (
    (lower.includes("model") && lower.includes("not found")) ||
    lower.includes("invalid model") ||
    (lower.includes("model") && lower.includes("does not exist")) ||
    (lower.includes("unknown") && lower.includes("model"))
  );
}

function formatApiError(status: number, errorText: string): string {
  let msg = `API error: ${status}`;
  try {
    const errorJson = JSON.parse(errorText) as any;
    if (typeof errorJson?.error === "string" && errorJson.error.trim()) {
      msg = errorJson.error.trim();
    }
    const detailsMsg = errorJson?.details?.error?.message;
    if (typeof detailsMsg === "string" && detailsMsg.trim()) {
      msg = `${msg} - ${detailsMsg.trim()}`;
    }
  } catch {
    const trimmed = (errorText || "").trim();
    msg = trimmed ? `${msg} - ${trimmed.slice(0, 600)}` : msg;
  }

  if (isQuotaExhaustedError(status, errorText)) {
    return `${QUOTA_EXHAUSTED_MARKER} ${msg}`;
  }
  if (isInvalidModelError(status, errorText)) {
    return `${INVALID_MODEL_MARKER} ${msg}`;
  }
  return msg;
}

export function isQuotaExhaustedMessage(message: string): boolean {
  return message.includes(QUOTA_EXHAUSTED_MARKER);
}

export function isInvalidModelMessage(message: string): boolean {
  return message.includes(INVALID_MODEL_MARKER);
}

function pickFallbackModel(current: string, attempted: Set<string>): string | null {
  const pool = getSelectedModels();
  const candidates = pool.filter(
    (model) => model && model !== current && !invalidModelCache.has(model) && !attempted.has(model)
  );
  if (candidates.length > 0) {
    return candidates[Math.floor(Math.random() * candidates.length)];
  }
  const generator = getGeneratorModel();
  if (generator && generator !== current && !invalidModelCache.has(generator) && !attempted.has(generator)) {
    return generator;
  }
  return null;
}

async function sleep(ms: number): Promise<void> {
  await new Promise((resolve) => setTimeout(resolve, ms));
}

async function fetchWithRetry(
  input: RequestInfo | URL,
  init: RequestInit,
  maxAttempts: number
): Promise<Response> {
  let lastResponse: Response | null = null;
  let lastError: unknown = null;

  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    try {
      const response = await fetch(input, init);
      lastResponse = response;

      if (response.ok) return response;

      if (!RETRYABLE_STATUS.has(response.status) || attempt === maxAttempts) {
        return response;
      }

      const retryAfterMs = parseRetryAfterMs(response);
      const base = response.status === 429 ? 1000 : 400;
      const jitter = Math.floor(Math.random() * 200);
      const backoffMs =
        (retryAfterMs !== null ? Math.min(15000, Math.max(0, retryAfterMs)) : base * 2 ** (attempt - 1)) +
        jitter;
      await sleep(backoffMs);
    } catch (err) {
      lastError = err;
      if (attempt === maxAttempts) break;
      const base = 400;
      const jitter = Math.floor(Math.random() * 200);
      const backoffMs = base * 2 ** (attempt - 1) + jitter;
      await sleep(backoffMs);
    }
  }

  if (lastResponse) return lastResponse;
  throw lastError instanceof Error ? lastError : new Error(String(lastError));
}

export function stripMarkdownCodeFences(text: string): string {
  let t = text.trim();

  if (t.startsWith("```")) {
    t = t.replace(/^```[a-zA-Z0-9_-]*\s*/m, "");
    t = t.replace(/\s*```\s*$/m, "");
  }

  return t.trim();
}

function stripJsonPrefix(text: string): string {
  const t = text.trimStart();
  if (/^json\s*[\[{]/i.test(t)) {
    return t.replace(/^json\s*/i, "");
  }
  return text;
}

function extractFirstJsonBlock(text: string): string | null {
  const startObj = text.indexOf("{");
  const startArr = text.indexOf("[");
  const start =
    startObj === -1 ? startArr : startArr === -1 ? startObj : Math.min(startObj, startArr);
  if (start === -1) return null;

  const opening = text[start];
  const expectedClosing = opening === "{" ? "}" : "]";

  let i = start;
  let depth = 0;
  let inString = false;
  let escaping = false;
  for (; i < text.length; i += 1) {
    const ch = text[i];

    if (inString) {
      if (escaping) {
        escaping = false;
        continue;
      }
      if (ch === "\\") {
        escaping = true;
        continue;
      }
      if (ch === '"') {
        inString = false;
      }
      continue;
    }

    if (ch === '"') {
      inString = true;
      continue;
    }
    if (ch === opening) {
      depth += 1;
      continue;
    }
    if (ch === expectedClosing) {
      depth -= 1;
      if (depth === 0) {
        return text.slice(start, i + 1);
      }
      continue;
    }

    if (opening === "{" && ch === "[") {
      depth += 1;
      continue;
    }
    if (opening === "{" && ch === "]") {
      depth = Math.max(0, depth - 1);
      if (depth === 0) {
        return text.slice(start, i + 1);
      }
      continue;
    }
    if (opening === "[" && ch === "{") {
      depth += 1;
      continue;
    }
    if (opening === "[" && ch === "}") {
      depth = Math.max(0, depth - 1);
      if (depth === 0) {
        return text.slice(start, i + 1);
      }
      continue;
    }
  }

  return null;
}

function normalizeJsonText(text: string): string {
  return text
    .replace(/[\u201C\u201D]/g, '"')
    .replace(/[\u2018\u2019]/g, "'")
    .replace(/,\s*([}\]])/g, "$1")
    .trim();
}

function escapeDanglingQuotesInStrings(text: string): string {
  let out = "";
  let inString = false;
  let escaping = false;

  const nextNonWs = (idx: number): string | null => {
    for (let j = idx; j < text.length; j += 1) {
      const c = text[j];
      if (!/\s/.test(c)) return c;
    }
    return null;
  };

  for (let i = 0; i < text.length; i += 1) {
    const ch = text[i];
    if (!inString) {
      if (ch === '"') inString = true;
      out += ch;
      continue;
    }

    if (escaping) {
      escaping = false;
      out += ch;
      continue;
    }

    if (ch === "\\") {
      escaping = true;
      out += ch;
      continue;
    }

    if (ch === '"') {
      const n = nextNonWs(i + 1);
      const isTerminator = n === null || n === "," || n === "}" || n === "]" || n === ":";
      if (isTerminator) {
        inString = false;
        out += ch;
        continue;
      }
      out += "\\\"";
      continue;
    }

    out += ch;
  }

  return out;
}

function parseJsonTolerant<T>(raw: string): T {
  const trimmed = stripJsonPrefix(stripMarkdownCodeFences(raw));
  const direct = normalizeJsonText(trimmed);
  try {
    return JSON.parse(direct) as T;
  } catch {
    // continue
  }

  const extracted = extractFirstJsonBlock(direct) ?? extractFirstJsonBlock(trimmed);
  if (!extracted) {
    throw new Error(`Failed to parse JSON response: ${raw}`);
  }

  const normalized = normalizeJsonText(extracted);
  try {
    return JSON.parse(normalized) as T;
  } catch {
    // continue
  }

  const repaired = escapeDanglingQuotesInStrings(normalized);
  try {
    return JSON.parse(repaired) as T;
  } catch {
    throw new Error(`Failed to parse JSON response: ${raw}`);
  }
}


export async function generateCompletion(
  options: GenerateOptions
): Promise<{ content: string; reasoning_details?: unknown; raw: ChatCompletionResponse }> {
  const maxTokens =
    typeof options.max_tokens === "number" && Number.isFinite(options.max_tokens)
      ? Math.max(16, Math.floor(options.max_tokens))
      : undefined;

  const cfg = getLlmConfigWithDefaults();
  const baseUrl = cfg.baseUrl;
  const apiKey = cfg.apiKey;
  let modelToUse = options.model || cfg.model;
  const attempted = new Set<string>();
  const headers: Record<string, string> = {
    "Content-Type": "application/json",
  };

  let response: Response;
  while (true) {
    if (invalidModelCache.has(modelToUse)) {
      const fallback = pickFallbackModel(modelToUse, attempted);
      if (fallback) modelToUse = fallback;
    }
    attempted.add(modelToUse);
    console.log("[LLM] generateCompletion:", {
      baseUrl,
      hasApiKey: !!apiKey,
      model: modelToUse,
    });

    response = await fetchWithRetry(
      "/api/chat",
      {
        method: "POST",
        headers: {
          ...headers,
        },
        body: JSON.stringify({
          baseUrl,
          apiKey,
          model: modelToUse,
          messages: options.messages,
          temperature: options.temperature ?? 0.7,
          max_tokens: maxTokens,
          ...(options.reasoning ? { reasoning: options.reasoning } : {}),
          ...(options.reasoning_effort ? { reasoning_effort: options.reasoning_effort } : {}),
          ...(options.response_format ? { response_format: options.response_format } : {}),
        }),
      },
      4
    );

    if (response.ok) break;
    const errorText = await response.text().catch(() => "");
    if (isInvalidModelError(response.status, errorText)) {
      invalidModelCache.add(modelToUse);
      const fallback = pickFallbackModel(modelToUse, attempted);
      if (fallback) {
        modelToUse = fallback;
        continue;
      }
    }
    throw new Error(formatApiError(response.status, errorText));
  }

  const result: ChatCompletionResponse = await response.json();
  const choice = result.choices?.[0];
  const assistantMessage = choice?.message;

  if (!assistantMessage) {
    throw new Error(
      `No response from model. Raw response: ${JSON.stringify(result).slice(0, 500)}`
    );
  }

  // Warn if output was truncated due to max_tokens
  if (choice.finish_reason === "length") {
    console.warn(
      `[LLM] Output truncated (finish_reason=length). Consider increasing max_tokens.`
    );
  }

  // 统计 AI 调用
  const inputChars = options.messages.reduce((sum, m) => {
    if (typeof m.content === "string") return sum + m.content.length;
    if (Array.isArray(m.content)) {
      return sum + m.content.reduce((s, p) => s + ("text" in p ? p.text.length : 0), 0);
    }
    return sum;
  }, 0);
  gameStatsTracker.addAiCall({
    inputChars,
    outputChars: assistantMessage.content.length,
    promptTokens: result.usage?.prompt_tokens,
    completionTokens: result.usage?.completion_tokens,
  });
  gameSessionTracker.addAiCall({
    inputChars,
    outputChars: assistantMessage.content.length,
    promptTokens: result.usage?.prompt_tokens,
    completionTokens: result.usage?.completion_tokens,
  });

  return {
    content: assistantMessage.content,
    reasoning_details: assistantMessage.reasoning_details,
    raw: result,
  };
}

export async function generateCompletionBatch(
  requests: GenerateOptions[]
): Promise<BatchCompletionResult[]> {
  if (!Array.isArray(requests) || requests.length === 0) return [];

  const cfg = getLlmConfigWithDefaults();
  const baseUrl = cfg.baseUrl;
  const apiKey = cfg.apiKey;
  const resolvedRequests = requests.map((r) => ({
    ...r,
    model: r.model || cfg.model,
  }));
  const headers: Record<string, string> = {
    "Content-Type": "application/json",
  };

  const response = await fetchWithRetry(
    "/api/chat",
    {
      method: "POST",
      headers,
      body: JSON.stringify({ baseUrl, apiKey, requests: resolvedRequests }),
    },
    3
  );

  if (!response.ok) {
    const errorText = await response.text().catch(() => "");
    throw new Error(formatApiError(response.status, errorText));
  }

  const data = await response.json();
  const results = Array.isArray(data?.results) ? data.results : [];

  return results.map((item: any) => {
    if (!item || item.ok !== true) {
      return {
        ok: false,
        error: String(item?.error || "Unknown error"),
        status: typeof item?.status === "number" ? item.status : undefined,
      };
    }
    const raw = item.data as ChatCompletionResponse;
    const choice = raw?.choices?.[0];
    const assistantMessage = choice?.message;
    if (!assistantMessage) {
      return { ok: false, error: "No response from model" };
    }
    return {
      ok: true,
      content: assistantMessage.content,
      reasoning_details: assistantMessage.reasoning_details,
      raw,
    };
  });
}

export async function* generateCompletionStream(
  options: GenerateOptions
): AsyncGenerator<string, void, unknown> {
  const maxTokens =
    typeof options.max_tokens === "number" && Number.isFinite(options.max_tokens)
      ? Math.max(16, Math.floor(options.max_tokens))
      : undefined;

  const cfg = getLlmConfigWithDefaults();
  const baseUrl = cfg.baseUrl;
  const apiKey = cfg.apiKey;
  let modelToUse = options.model || cfg.model;
  const attempted = new Set<string>();
  const headers: Record<string, string> = {
    "Content-Type": "application/json",
  };

  let response: Response;
  while (true) {
    if (invalidModelCache.has(modelToUse)) {
      const fallback = pickFallbackModel(modelToUse, attempted);
      if (fallback) modelToUse = fallback;
    }
    attempted.add(modelToUse);
    response = await fetchWithRetry(
      "/api/chat",
      {
        method: "POST",
        headers: {
          ...headers,
        },
        body: JSON.stringify({
          baseUrl,
          apiKey,
          model: modelToUse,
          messages: options.messages,
          temperature: options.temperature ?? 0.7,
          max_tokens: maxTokens,
          stream: true,
          ...(options.reasoning ? { reasoning: options.reasoning } : {}),
          ...(options.reasoning_effort ? { reasoning_effort: options.reasoning_effort } : {}),
          ...(options.response_format ? { response_format: options.response_format } : {}),
        }),
      },
      4
    );

    if (response.ok) break;
    const errorText = await response.text().catch(() => "");
    if (isInvalidModelError(response.status, errorText)) {
      invalidModelCache.add(modelToUse);
      const fallback = pickFallbackModel(modelToUse, attempted);
      if (fallback) {
        modelToUse = fallback;
        continue;
      }
    }
    throw new Error(formatApiError(response.status, errorText));
  }

  const reader = response.body?.getReader();
  if (!reader) {
    throw new Error("No response body");
  }

  const decoder = new TextDecoder();
  let buffer = "";
  let totalOutputChars = 0;

  // 计算输入字符数
  const inputChars = options.messages.reduce((sum, m) => {
    if (typeof m.content === "string") return sum + m.content.length;
    if (Array.isArray(m.content)) {
      return sum + m.content.reduce((s, p) => s + ("text" in p ? p.text.length : 0), 0);
    }
    return sum;
  }, 0);

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    buffer += decoder.decode(value, { stream: true });
    const lines = buffer.split("\n");
    buffer = lines.pop() || "";

    for (const line of lines) {
      const trimmed = line.trim();
      if (!trimmed || trimmed === "data: [DONE]") continue;
      if (!trimmed.startsWith("data: ")) continue;

      try {
        const json = JSON.parse(trimmed.slice(6));
        const delta = json.choices?.[0]?.delta?.content;
        if (delta) {
          totalOutputChars += delta.length;
          yield delta;
        }
      } catch {
        // Skip malformed JSON
      }
    }
  }

  // 流式结束后统计 AI 调用
  gameStatsTracker.addAiCall({
    inputChars,
    outputChars: totalOutputChars,
  });
  gameSessionTracker.addAiCall({
    inputChars,
    outputChars: totalOutputChars,
  });
}

export async function generateJSON<T>(
  options: GenerateOptions & { schema?: string }
): Promise<T> {
  const messagesWithFormat = [...options.messages];

  const lastMessage = messagesWithFormat[messagesWithFormat.length - 1];
  if (lastMessage && lastMessage.role === "user") {
    const suffix =
      "\n\nRespond with valid JSON only. No markdown, no code blocks, just raw JSON. If you need to include double quotes inside string values, escape them as \\\".";
    if (typeof lastMessage.content === "string") {
      lastMessage.content += suffix;
    } else if (Array.isArray(lastMessage.content)) {
      const parts = lastMessage.content;
      const lastPart = parts[parts.length - 1];
      if (lastPart && lastPart.type === "text") {
        lastPart.text += suffix;
      } else {
        parts.push({ type: "text", text: suffix });
      }
    }
  }

  const result = await generateCompletion({
    ...options,
    messages: messagesWithFormat,
  });

  return parseJsonTolerant<T>(result.content);
}
